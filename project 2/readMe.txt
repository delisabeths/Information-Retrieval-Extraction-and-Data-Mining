Project 1 Group 7

Mingyang Zheng (mz2594)   Huafeng Shi (hs2917)

1. File List
    (1) code
        App.java: the entrance of the program, and the basic logic of the program
        ISearchService.java: use google cumstom search API to search by query; will return 10 results each time
        AnnotationService.java: use  Stanford CoreNLP software suite to do two round annotation
        util.WebPageParser.java: use jsoup to parse website and remove tags
        model.RelationEntity.java: the model, containing fields entityValue1, entityValue2, relationType, entityType1, entityType2, confidence, visited;
        
    (2) transcript: the runs of your program on the test case: 4 0.35 "bill gates microsoft" 10 
    (3) readMe.txt: the README file


2. Run the program

    (1) prerequisite: 
        $ sudo apt-get install unzip
		$ unzip cs6111project2.zip
		$ sudo apt-get install default-jdk
		$ sudo apt-get install maven
    
    
    (2) run the program:
    	$ mvn install

        $ java -jar target/cs6111project2-1.0-SNAPSHOT.jar
        In this situation, the program will use the default api_key 
        (AIzaSyAHzQAbQFJmGyElhnh_VVFay_ECunRqVoE), engine id (009650898989487274447:ghd3zgarfa4), relation number(4), threshold (0.35), query ("bill gates microsoft"), output number (10)

         $ java -jar target/cs6111project2-1.0-SNAPSHOT.jar {api_key} {engine id} {relation number} {threshold} {query} {output number}
         use above command to specify the parameter you want.
         for example:
         $ java -jar target/cs6111project2-1.0-SNAPSHOT.jar AIzaSyAHzQAbQFJmGyElhnh_VVFay_ECunRqVoE 009650898989487274447:ghd3zgarfa4 4 0.35 "bill gates microsoft" 10 


3. Internal design
    step 1: Initialize List<RelationEntity> result, the List of unduplicated extracted tuples, as the empty List.
    step 2: Initialize the relavant services, including searchService, annotationService.
    step 3: Query Google Custom Search Engine to obtain the URLs for the top-10 webpages for seed query 
    step 4: For each URL from the previous step that has not been processed before (skip already-seen URLs, use Set<String> visitedLinks to stored visited links)
    	a. Retrieve the corresponding webpage; if cannot retrieve the webpage (e.g., because of a timeout), just skip it and move on.
		b. Extract the actual plain text from the webpage using Jsoup and other Regular expression.
		c. Annotate the text with the Stanford CoreNLP software suite and, in particular, with the Stanford Relation Extractor, to extract all instances of the relation specified by input parameter relation number r. We only consider an extracted relation to be an instance of r if r has the highest extraction confidence among all relation types.
		d. Identify the tuples for r that have an associated extraction confidence of at least threshold and add them to List result.
    step 5: Remove exact duplicates from List result: if result contains tuples that are identical to each other, keep only the copy that has the highest extraction confidence and remove from result the duplicate copies. 
	step 6: sort in decreasing order by extraction confidence
		Collections.sort(result, new Comparator<RelationEntity>() {
                public int compare(RelationEntity o1, RelationEntity o2) {
                    return (int) Math.signum(o2.getConfidence() - o1.getConfidence());
                }
            });
    step 7: If result contains at least k tuples, return the top-k such tuples sorted in decreasing order by extraction confidence, together with the extraction confidence of each tuple, and stop.
    step 8: Otherwise, select from result a tuple y such that (1) y has not been used for querying yet and (2) y has an extraction confidence that is highest among the tuples in result that have not yet been used for querying. Create a query q from tuple y by just concatenating the attribute values together, and go to Step 3. If no such y tuple exists, then stop. (ISE has "stalled" before retrieving k high-confidence tuples.)
    Whether the tuple has been used is recorded by RelationEntity.isVisited(). After using this tuple, we set relationEntity.setVisited(true)


4. realization details
	(1) website parsing: 
	firstly, we only retain the contents in the <p> tag hat do not contain <option> tag in it. Then we replace all the tags in <p> with white space. Next, we only retain number, English character, Punctuation and white space for annotation.
	Then we remove Superscript like "[2]", because this will prevent the Stanford CoreNLP software suite from split sentences correctly. Finaly, we remove "&nbsp;" "&amp;" generated by Jsoup.

	(2) Two round annotations: 
	Relation annotation requires six annotators, namely, tokenize, ssplit, pos, lemma, ner, and parse. Unfortunately, the parse annotator is computationally expensive, so for efficiency you need to minimize its use. Specifically, you should not run parse over sentences that do not contain named entities of the right type for the relation of interest r. The required named entities for each relation type are as follows:
		Live_In: people and location
		Located_In: two locations
		OrgBased_In: organization and location
		Work_For: organization and people
	So to annotate the text, we should implement two pipelines. we should run the first pipeline, which consists of tokenize, ssplit, pos, lemma, and ner, for the full text that you extracted from a webpage. The output will identify the sentences in the webpage text together with the named entities, if any, that appear in each sentence.
	To identify the sentences in the webpage text together with the named entities, we use a map to count the number of apperance of the named entities in the sentence. 
		Initally, we set the required number of apperance of the named entities for the relation.
			private void mapReset() {
		        if (relationType.equals("Live_In")) {
		            map.put("PERSON", 1);
		            map.put("LOCATION",1);
		            map.put("ORGANIZATION", 0);
		        } else if (relationType.equals("Located_In")) {
		            map.put("PERSON", 0);
		            map.put("LOCATION",2);
		            map.put("ORGANIZATION", 0);
		        } else if (relationType.equals("OrgBased_In")) {
		            map.put("PERSON", 0);
		            map.put("ORGANIZATION", 1);
		            map.put("LOCATION",1);
		        } else if (relationType.equals("Work_For")) {
		            map.put("PERSON", 1);
		            map.put("LOCATION",0);
		            map.put("ORGANIZATION", 1);
		        }
		    }
		Then every time the named entity appears in the sentence, we substract 1. If there are enough named entities appearing in the sentence, we retain the sentence for the second round of annotation.
			List<CoreMap> sentences = annotation.get(CoreAnnotations.SentencesAnnotation.class);

	        StringBuilder sb = new StringBuilder();;

	        for(CoreMap sentence: sentences) {

	            mapReset();

	            for (CoreLabel token : sentence.get(CoreAnnotations.TokensAnnotation.class)) {
	                String label = token.get(CoreAnnotations.NamedEntityTagAnnotation.class);
	                if (map.containsKey(label)) {
	                    map.put(label, map.get(label) - 1);
	                }
	            }
	            if (map.get("PERSON") <= 0 && map.get("LOCATION") <= 0 && map.get("ORGANIZATION") <= 0) {
	                sb.append(sentence.toString());
	                sb.append(" ");
	            }
	        }

	Then, we should run the second pipeline, which includes the expensive parse annotator, separately over each sentence that contains the required named entities for the relation of interest, as specified above. Note that the two named entities might appear in either order in a sentence and this is fine. The second pipeline consists of tokenize, ssplit, pos, lemma, ner, and parse.

	(3) Duplication removal:
	In Class model.RelationEntity we difined, we override hashcode() and equals() method to define whether two relationEntities are duplicated. 
	    @Override
	    public boolean equals(Object obj) {
	        if (obj == null)
	            return false;
	        if (obj instanceof RelationEntity) {
	            RelationEntity relationEntity = (RelationEntity) obj;
	            if (this.relationType.equals(relationEntity.getRelationType()) && (this.entityValue1.equals(relationEntity.getEntityValue1()) && this.entityValue2.equals(relationEntity.getEntityValue2())
	                    || this.entityValue2.equals(relationEntity.getEntityValue1()) && this.entityValue1.equals(relationEntity.getEntityValue2()))) {
	                return true;
	            }
	        }
	        return false;

	    }

	    @Override
	    public int hashCode() {
	        int result = getEntityValue1().hashCode() + getEntityValue2().hashCode() + getRelationType().hashCode();
	        return result;
	    }


5. api_key: AIzaSyAHzQAbQFJmGyElhnh_VVFay_ECunRqVoE, 
   engine id: 009650898989487274447:ghd3zgarfa4

   






